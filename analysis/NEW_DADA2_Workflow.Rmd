---
title: "New_DADA2_Workflow"
author: "Yining Sun"
date: "2023-04-25"
output: html_document
editor_options: 
  chunk_output_type: console
---

# Let's strat! Set workdir, source code and load packages!
```{r prep-to-start}
# be in the right place
setwd("/local/workdir/ys636/BIOMI6300-Project2")

# source functions.R
source("/local/workdir/ys636/BIOMI6300-Project2/code/functions.R")

# load packages with pacman
pacman::p_load(dada2, tidyverse, phyloseq, patchwork, Biostrings, install = FALSE)

# check dada2 version
packageVersion("dada2")
```



# Set path
```{r set-path}

# set paths for meat and dairy folders of gzipped files
path <- ("/local/workdir/ys636/BIOMI6300-Project2/data")

# check path
list.files(path)

```

# Create variables for forward and reverse reads
```{r F-and-R-variables}
#R1 is forward, R2 is reverse
# F and R variables
F_reads <- sort(list.files(path, pattern = "_1.fastq",
                                 full.names = TRUE))

R_reads <- sort(list.files(path, pattern = "_2.fastq",
                                 full.names = TRUE))

```

# Quality plot of raw reads
```{r QualPlot-untrimmed}
# show the quality of each base on the reads of all the 8 samples in both groups

## Forward
F_Qual_plot <- plotQualityProfile(F_reads[1:4])
F_Qual_plot                     ### The first 260 bases had quality scores over 30
## Reverse
R_Qual_plot <- plotQualityProfile(R_reads[1:4])
R_Qual_plot                      ### The first 200ish bases had quality scores over 30

```


# Modify file names for the forward and reverse filtered reads
```{r variables-trimmed-reads}

# Create characters for trimmed reads 
sample <- scan(file = "/local/workdir/ys636/BIOMI6300-Project2/data/name.txt", what = "character")
sample

# create variables to hold file names
filtered_F_reads <- file.path(path, "filtered", paste0(sample, "_1_filtered.fastq"))
filtered_R_reads <- file.path(path, "filtered", paste0(sample, "_2_filtered.fastq"))

```


# Filter and trim the raw reads!!
```{r filter-and-trim}

filtered_out <- filterAndTrim(F_reads, filtered_F_reads, 
                              R_reads, filtered_R_reads,
                              truncLen = 0, trimRight = 100,
                              maxN = 0, maxEE = c(1,1), truncQ = 2, 
                              rm.phix = TRUE, compress = TRUE, 
                              multithread = TRUE)
## kept the first 200 bases in forward reads and the first 200 bases in reverse reads

```


# Plot the quality of filtered and trimmed reads for both groups
```{r QualPlot-trim}

## forward
filtered_F_Qual_plot <- plotQualityProfile(filtered_F_reads[1:4])
filtered_F_Qual_plot  
## reverse
filtered_R_Qual_plot <- plotQualityProfile(filtered_R_reads[1:4])
filtered_R_Qual_plot

```


# Backup plan: make a workdir for filtered F reads only!
```{r filter-F}
# make a workdir called "filtered_F"
filtered_F_reads_only <- file.path(path, "filtered_F", paste0(sample, "_1_filtered.fastq"))

# filter and trim F reads
filtered_out_F <- filterAndTrim(F_reads, filtered_F_reads_only, 
                              truncLen = 0, trimRight = 40, 
                              # since only trimming forward reads, I can only trim right 40 bases.
                              maxN = 0, maxEE = 1, truncQ = 2, 
                              rm.phix = TRUE, compress = TRUE, 
                              multithread = TRUE)


### annotation: when doing both forward and reverse reads, there might be no merged pairs.
### if allowing concatenation, there would be "N"s, 
### which are not acceptable by AssignSpecies function

```


# Error model
```{r learn-errors}

# learn errors
err_forward_reads <- learnErrors(filtered_F_reads, multithread = TRUE)
err_reverse_reads <- learnErrors(filtered_R_reads, multithread = TRUE)

# plot the errors
plotErrors(err_forward_reads, nominalQ = TRUE)
plotErrors(err_reverse_reads, nominalQ = TRUE)

```


# Infer ASVs on the forward and reverse seqs with DADA2
```{r infer-ASVs}
# run dada2 on the forward reads
dada_F <- dada(filtered_F_reads, err = err_forward_reads, multithread =  TRUE)
dada_F
# run dada2 on the reverse reads
dada_R <- dada(filtered_R_reads, err = err_reverse_reads, multithread =  TRUE)
dada_R

### Forward reads all have more than 80 sequence variants 
### Some of the reverse reads only have less than 10 sequence variants

```


# Merge forward and reverse ASVs
```{r merge-FandR-ASVs}

# Attention: allows concatenation here
merged_amplicons <- mergePairs(dada_F, filtered_F_reads,
                               dada_R, filtered_R_reads,
                               verbose = TRUE, justConcatenate = TRUE)

### average merged pairs: 10,000
### I will proceed with this list


# Let's try "justConcateate = FALSE"
merged_amplicons_noConcatenate <- mergePairs(dada_F, filtered_F_reads,
                               dada_R, filtered_R_reads,
                               verbose = TRUE, justConcatenate = FALSE)

### average merged pairs: ~3


### For continuous steps, I would try:
### 1. assign taxonomy and species only using forward reads, 
###    since this dada2 function does not accept "N"s in sequence,
###    (there will be "N"s if allowing concatenation)
### 2. proceed with concatenated amplicon sequences - merged_amplicons


```


# Generate count tables
```{r count-tab}

seqtab <- makeSequenceTable(merged_amplicons)
class(seqtab)      ### "matrix" "array"
typeof(seqtab)     ### "integer"
dim(seqtab)        ### 16 rows, 45935 columns
View(seqtab)       ### ASV clusters existed in specific samples


# inspect the distribution of sequence lengths of all ASVs in dataset
table(nchar(getSequences(seqtab))) 
### 408  409  410  411  412
### 183  74 13434 28739 3505

```


# Check and remove for Chimeras (Bimeras)
```{r check-chimeras}

# identify and remove chimeras
seqtab_nochim <- removeBimeraDenovo(seqtab, verbose = TRUE)  
### Identified 42953 bimeras out of 45935 input sequences.

chim_check <- sum(seqtab_nochim)/sum(seqtab)*100 # 58.2609% of counts were not chimeras.
frac_removed <- (1-chim_check)*100 ### 41.73903% of counts were chimeras and removed.

```


# Track the sequences through the pipeline
```{r seq-track}

# generate a function to identify number seqs
getN <- function(x) sum(getUniques(x))

# make the table to check the seqs
track <- cbind(filtered_out,
               sapply(dada_F, getN),
               sapply(dada_R, getN),
               sapply(merged_amplicons, getN),
               rowSums(seqtab_nochim))

head(track)

# change column names
colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nochim")

head(track)

rownames(track) <- sample

track


### Merged amplicon data look good so far!

```



# Generate a plot to track the reads through our DADA2 pipeline
```{r track-plot}
# Meat group
track %>%
  as.data.frame() %>%
  rownames_to_column(var = "names") %>%
  pivot_longer(input:nochim, names_to = "read_type", values_to = "num_reads") %>%
  make_MA_metadata() %>%
  mutate(read_type = fct_relevel(read_type, "input", "filtered", "denoisedF", "denoisedR",
                                 "merged", "nochim")) %>%
  ggplot(aes(x = read_type, y = num_reads, fill = read_type)) + 
  facet_grid(~fraction) +
  geom_line(aes(group = names), color = "red") +
  geom_point(shape = 21, size = 3, alpha = 0.8) +
  scale_fill_brewer(palette = "Spectral") +
  theme_bw() +
  labs(x = "Filtering Step", y = "Number of Sequences") +
  theme(legend.position = "bottom", legend.title = element_blank(),
        axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))


### The plot showed that after filtering and removing chimeras
### 12 samples have a few sequences left.
### Plan to try count table with the forward reads only next.

```


# Steps to track the sequences through the dada2 pipeline
```{r count-tab}
# Generate count table for forward reads
seqtab_F <- makeSequenceTable(dada_F)
class(seqtab_F)      ### "matrix" "array"
typeof(seqtab_F)     ### "integer"
dim(seqtab_F)        ### 16 rows, 3460 columns
View(seqtab_F)       ### ASV clusters existed in specific samples

# inspect the distribution of sequence lengths of all ASVs in dataset
table(nchar(getSequences(seqtab_F))) 
### 198  199  200  201
### 11    28  1077 2344


# Check and remove chimeras - does forward read have chimeras?
seqtab_nochim_F <- removeBimeraDenovo(seqtab_F, verbose = TRUE)
### Identified 2301 bimeras out of 3460 input sequences.
chim_check_F <- sum(seqtab_nochim_F)/sum(seqtab_F) # 73.5614% of counts were not chimeras.
frac_removed_F <- (1 - chim_check_F)*100 ### 26.4386% of counts were chimeras and removed.


# Track the sequences in the pipeline
track_F <- cbind(filtered_out,
               sapply(dada_F, getN),
               rowSums(seqtab_nochim_F))

head(track_F)

# change column names
colnames(track_F) <- c("input", "filtered", "denoisedF", "nochim")

head(track_F)

rownames(track_F) <- sample

track_F


# Plot the sequences in the track 
track_F %>%
  as.data.frame() %>%
  rownames_to_column(var = "names") %>%
  pivot_longer(input:nochim, names_to = "read_type", values_to = "num_reads") %>%
  make_MA_metadata() %>%
  mutate(read_type = fct_relevel(read_type, "input", "filtered", "denoisedF", "nochim")) %>%
  ggplot(aes(x = read_type, y = num_reads, fill = read_type)) + 
  facet_grid(~fraction) +
  geom_line(aes(group = names), color = "red") +
  geom_point(shape = 21, size = 3, alpha = 0.8) +
  scale_fill_brewer(palette = "Spectral") +
  theme_bw() +
  labs(x = "Filtering Step", y = "Number of Sequences") +
  theme(legend.position = "bottom", legend.title = element_blank(),
        axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))


### The forward reads showed the same trends as the pipeline proceeds
### Continue to assign taxa and species to see if which type of data work better.

```


# Assign taxonomy to merged pairs
```{r assign-tax}

taxa <- assignTaxonomy(seqtab_nochim, "/workdir/in_class_data/taxonomy/silva_nr99_v138.1_train_set.fa.gz", multithread=FALSE)


# addSpecies() does not accept "N"s
# taxa <- addSpecies(taxa, "/workdir/in_class_data/taxonomy/silva_species_assignment_v138.1.fa.gz")


# Inspect the taxonomy 
taxa_print <- taxa # Removing sequence rownames for display only
rownames(taxa_print) <- NULL
View(taxa_print)
```


# Assign taxonomy to forward reads
```{r assign-tax-F}

taxa_F <- assignTaxonomy(seqtab_nochim_F, "/workdir/in_class_data/taxonomy/silva_nr99_v138.1_train_set.fa.gz", multithread=FALSE)


taxa_F <- addSpecies(taxa_F, "/workdir/in_class_data/taxonomy/silva_species_assignment_v138.1.fa.gz")


# Inspect the taxonomy 
taxa_F_print <- taxa_F # Removing sequence rownames for display only
rownames(taxa_F_print) <- NULL
View(taxa_F_print)

### In the "Species" column, there are all "NA", although there was no error when executing the function.
### Will skip "addSpecies" and proceed with merged pairs.

```


# Skipped evaluating accuracyas there was no mock community in my samples


# Prepare the data for export

## 1. ASV Table
```{r prepare-ASV-table}
# prep the asv table
samples_out <- rownames(seqtab_nochim)

# pull out sample names from the file name
sample_names_reformatted <- sapply(strsplit(samples_out, split = "_1"), `[`, 1)

# replace the names in our seqtable
rownames(seqtab_nochim) <- sample_names_reformatted

View(seqtab_nochim)

### intuition check
stopifnot(rownames(seqtab_nochim) == sample_names_reformatted)

################ modify the ASV names and then save a fasta file
# give headers more manageable names
# first pull the ASV sequences
asv_seqs <- colnames(seqtab_nochim)

# make headers for our seq fasta file, which will be our ASV Names
asv_headers <- vector(dim(seqtab_nochim)[2], mode = "character")

# loop through vector and fill it in with ASV names
for (i in 1:dim(seqtab_nochim)[2]) {
  asv_headers[i] <- paste(">ASV", i, sep = "_")
}

# intuition check
asv_headers


### rename ASVs in table and then write out out ASV fasta file
asv_tab <- t(seqtab_nochim)
View(asv_tab)

# rename ASVs
row.names(asv_tab) <- sub(">", "", asv_headers)
View(asv_tab)

# write the count table to a file
write.table(asv_tab, "./data/ASV_counts.tsv", sep = "\t", quote = FALSE, col.names = NA)

# write out the fasta file for reference later on for what seq matches what ASV
asv_fasta <- c(rbind(asv_headers, asv_seqs))

# save to a file
write(asv_fasta, "data/ASV.fasta")


```



## 2. taxonomy table
```{r prep-taxonomy-tab}
View(taxa)

#### prep taxonomy table
# add ASV seqs from the rownames to a column
new_tax_tab <- taxa %>%
  as.data.frame() %>%
  rownames_to_column(var = "ASVseqs")
head(new_tax_tab)

# intuition check
stopifnot(new_tax_tab$ASVseqs == colnames(seqtab_nochim))

# add the ASV names
rownames(new_tax_tab) <- rownames(asv_tab)
View(new_tax_tab)

### final prep of tax table, add new column with ASV names
asv_tax <-
  new_tax_tab %>%
  # add rownames from count table for phyloseq handoff
  mutate(ASV = rownames(asv_tab)) %>%
  # resort the columns with select
  dplyr::select(Kingdom, Phylum, Class, Order, Family, Genus, ASV, ASVseqs)

View(asv_tax)

# intuition check 
stopifnot(asv_tax$ASV == rownames(asv_tax), rownames(asv_tax) == rownames(asv_tab))

# write the table
write.table(asv_tax, "data/ASV_taxonomy.tsv", sep = "\t", quote = FALSE, col.names = NA)

```





