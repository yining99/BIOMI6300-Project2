---
title: "New_DADA2_Workflow"
author: "Yining Sun"
date: "2023-04-25"
output: html_document
editor_options: 
  chunk_output_type: console
---

# Let's strat! Set workdir, source code and load packages!
```{r prep-to-start}
# be in the right place
setwd("/local/workdir/ys636/BIOMI6300-Project2")

# source functions.R
source("/local/workdir/ys636/BIOMI6300-Project2/code/functions.R")

# load packages with pacman
pacman::p_load(dada2, tidyverse, phyloseq, patchwork, Biostrings, install = FALSE)

# check dada2 version
packageVersion("dada2")
```



# Set path
```{r set-path}

# set paths for meat and dairy folders of gzipped files
path <- ("/local/workdir/ys636/BIOMI6300-Project2/data")

# check path
list.files(path)

```

# Create variables for forward and reverse reads
```{r F-and-R-variables}
#R1 is forward, R2 is reverse
# F and R variables
F_reads <- sort(list.files(path, pattern = "_1.fastq",
                                 full.names = TRUE))

R_reads <- sort(list.files(path, pattern = "_2.fastq",
                                 full.names = TRUE))

```

# Quality plot of raw reads
```{r QualPlot-untrimmed}
# show the quality of each base on the reads of all the 8 samples in both groups

## Forward
F_Qual_plot <- plotQualityProfile(F_reads[1:4])
F_Qual_plot                     ### The first 260 bases had quality scores over 30
## Reverse
R_Qual_plot <- plotQualityProfile(R_reads[1:4])
R_Qual_plot                      ### The first 200ish bases had quality scores over 30

```


# Modify file names for the forward and reverse filtered reads
```{r variables-trimmed-reads}

# Create characters for trimmed reads 
sample <- scan(file = "/local/workdir/ys636/BIOMI6300-Project2/data/name.txt", what = "character")
sample

# create variables to hold file names
filtered_F_reads <- file.path(path, "filtered", paste0(sample, "_1_filtered.fastq"))
filtered_R_reads <- file.path(path, "filtered", paste0(sample, "_2_filtered.fastq"))

```


# Filter and trim the raw reads!!
```{r filter-and-trim}

filtered_out <- filterAndTrim(F_reads, filtered_F_reads, 
                              R_reads, filtered_R_reads,
                              truncLen = 0, trimRight = 100,
                              maxN = 0, maxEE = c(1,1), truncQ = 2, 
                              rm.phix = TRUE, compress = TRUE, 
                              multithread = TRUE)
## kept the first 200 bases in forward reads and the first 200 bases in reverse reads

```


# Plot the quality of filtered and trimmed reads for both groups
```{r QualPlot-trim}

## forward
filtered_F_Qual_plot <- plotQualityProfile(filtered_F_reads[1:4])
filtered_F_Qual_plot  
## reverse
filtered_R_Qual_plot <- plotQualityProfile(filtered_R_reads[1:4])
filtered_R_Qual_plot

```


# Backup plan: make a workdir for filtered F reads only!
```{r filter-F}
# make a workdir called "filtered_F"
filtered_F_reads_only <- file.path(path, "filtered_F", paste0(sample, "_1_filtered.fastq"))

# filter and trim F reads
filtered_out_F <- filterAndTrim(F_reads, filtered_F_reads_only, 
                              truncLen = 0, trimRight = 40, 
                              # since only trimming forward reads, I can only trim right 40 bases.
                              maxN = 0, maxEE = 1, truncQ = 2, 
                              rm.phix = TRUE, compress = TRUE, 
                              multithread = TRUE)


### annotation: when doing both forward and reverse reads, there might be no merged pairs.
### if allowing concatenation, there would be "N"s, 
### which are not acceptable by AssignSpecies function

```


# Error model
```{r learn-errors}

# learn errors
err_forward_reads <- learnErrors(filtered_F_reads, multithread = TRUE)
err_reverse_reads <- learnErrors(filtered_R_reads, multithread = TRUE)

# plot the errors
plotErrors(err_forward_reads, nominalQ = TRUE)
plotErrors(err_reverse_reads, nominalQ = TRUE)

```


# Infer ASVs on the forward and reverse seqs with DADA2
```{r infer-ASVs}
# run dada2 on the forward reads
dada_F <- dada(filtered_F_reads, err = err_forward_reads, multithread =  TRUE)
dada_F
# run dada2 on the reverse reads
dada_R <- dada(filtered_R_reads, err = err_reverse_reads, multithread =  TRUE)
dada_R

### Forward reads all have more than 80 sequence variants 
### Some of the reverse reads only have less than 10 sequence variants

```


# Merge forward and reverse ASVs
```{r merge-FandR-ASVs}

# Attention: allows concatenation here
merged_amplicons <- mergePairs(dada_F, filtered_F_reads,
                               dada_R, filtered_R_reads,
                               verbose = TRUE, justConcatenate = TRUE)

### average merged pairs: 10,000
### I will proceed with this list


# Let's try "justConcateate = FALSE"
merged_amplicons_noConcatenate <- mergePairs(dada_F, filtered_F_reads,
                               dada_R, filtered_R_reads,
                               verbose = TRUE, justConcatenate = FALSE)

### average merged pairs: ~3


### For continuous steps, I would try:
### 1. assign taxonomy and species only using forward reads, 
###    since this dada2 function does not accept "N"s in sequence,
###    (there will be "N"s if allowing concatenation)
### 2. proceed with concatenated amplicon sequences - merged_amplicons


```


# Generate count tables
```{r count-tab}

seqtab <- makeSequenceTable(merged_amplicons)
class(seqtab)      ### "matrix" "array"
typeof(seqtab)     ### "integer"
dim(seqtab)        ### 16 rows, 45935 columns
View(seqtab)       ### ASV clusters existed in specific samples


# inspect the distribution of sequence lengths of all ASVs in dataset
table(nchar(getSequences(seqtab))) 
### 408  409  410  411  412
### 183  74 13434 28739 3505

```


# Check and remove for Chimeras (Bimeras)
```{r check-chimeras}

# identify and remove chimeras
seqtab_nochim <- removeBimeraDenovo(seqtab, verbose = TRUE)  
### Identified 42953 bimeras out of 45935 input sequences.

chim_check <- sum(seqtab_nochim)/sum(seqtab)*100 # 58.2609% of counts were not chimeras.
frac_removed <- (1-chim_check)*100 ### 41.73903% of counts were chimeras and removed.

```


# Track the sequences through the pipeline
```{r seq-track}

# generate a function to identify number seqs
getN <- function(x) sum(getUniques(x))

# make the table to check the seqs
track <- cbind(filtered_out,
               sapply(dada_F, getN),
               sapply(dada_R, getN),
               sapply(merged_amplicons, getN),
               rowSums(seqtab_nochim))

head(track)

# change column names
colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nochim")

head(track)

rownames(track) <- sample

track


### Merged amplicon data look good so far!


```
